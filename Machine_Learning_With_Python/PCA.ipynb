{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **How to select important eigenvalues (or components) in Principal Component Analysis (PCA)**\n",
        "\n",
        "The scree plot, Kaiser criteria, and variance plot are crucial tools in PCA for selecting the principal components to retain.\n",
        "\n",
        "# **Scree Plot**\n",
        "\n",
        "  **Purpose**: Visual method to assess the importance of each principal component.\n",
        "  **Construction**: Plot the eigenvalues of the components in descending order against their corresponding component number.\n",
        "\n",
        "  **Interpretation**: Look for an \"elbow\" or a point where the slope of the plot changes drastically from steep to shallow. The components before this elbow are considered significant because they explain a large portion of the variance, while those after contribute little additional explanatory power.\n",
        "\n",
        "  **Role**: Provides a straightforward and intuitive way to visualize the diminishing returns of adding more components.\n",
        "\n",
        "# **Kaiser Criteria**\n",
        "  **Purpose**: Quantitative rule to retain significant components.\n",
        "  **Rule**: Retain components with eigenvalues greater than 1.\n",
        "\n",
        "  **Rationale**: An eigenvalue greater than 1 indicates that the component explains more variance than an individual original variable, justifying its retention.\n",
        "\n",
        "  **Role**: Offers a simple and objective criterion for selecting components, helping to avoid retaining components that do not add substantial explanatory power.\n",
        "\n",
        "# **Variance Plot (Cumulative Variance Explained)**\n",
        "\n",
        "  **Purpose**: Quantifies the total variance explained by the selected components.\n",
        "  **Construction**: Plot the cumulative proportion of total variance explained by the components against the number of components.\n",
        "\n",
        "  **Interpretation**: Determine the number of components needed to reach a predetermined threshold of explained variance (e.g., 90% or 95%). This helps in ensuring that enough variance is captured by the retained components while reducing dimensionality.\n",
        "\n",
        "  **Role**: Ensures that the retained components capture a sufficient proportion of the dataâ€™s variance, aiding in maintaining the integrity of the data representation.\n",
        "\n",
        "**How They Complement Each Other**\n",
        "\n",
        "  Scree Plot and Kaiser Criteria: The scree plot gives a visual indication which can be supported by the Kaiser criteria. For example, you may see an elbow at the 4th component in the scree plot and find that the first four components have eigenvalues greater than 1.\n",
        "\n",
        "  Variance Plot and Kaiser Criteria: While the Kaiser criteria ensure that only components with substantial variance are kept, the variance plot ensures that collectively, these components explain an adequate amount of total variance.\n",
        "\n",
        " ** Scree Plot and Variance Plot:** The elbow point in the scree plot can be cross-validated with the variance plot to ensure that the components before the elbow capture most of the variance."
      ],
      "metadata": {
        "id": "UPcynBKvONku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_qcHN_VjhyI",
        "outputId": "e5d744da-ac91-43f8-ed51-441c2ab393b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "Accuracy: 0.90\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "\n",
        "# Load the Iris dataset from scikit-learn's datasets module. X contains the feature data and y contains the target variable\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Separate the DataFrame into features (X) and target variable (y).\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to DataFrame for consistency with previous examples\n",
        "data = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "data['target'] = y\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(data.head())\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('target', axis=1)  # All columns except 'target'\n",
        "y = data['target']  # The 'target' column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# Split the data into training and testing sets with an 80-20 split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "# Standardize the features to have zero mean and unit variance.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA\n",
        "# Apply PCA to reduce the dataset to 2 principal components for simplicity\n",
        "pca = PCA(n_components=2)  # Reduce to 2 principal components for simplicity\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train a classifier on the reduced data\n",
        "# Train a RandomForestClassifier on the PCA-transformed training data\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "# Predict the target variable on the PCA-transformed test data and evaluate the classifier's performance, printing the accuracy\n",
        "y_pred = clf.predict(X_test_pca)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to DataFrame for consistency with previous examples\n",
        "data = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "data['target'] = y\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('target', axis=1)  # All columns except 'target'\n",
        "y = data['target']  # The 'target' column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a classifier without PCA\n",
        "clf_no_pca = RandomForestClassifier(random_state=42)\n",
        "clf_no_pca.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions without PCA\n",
        "y_pred_no_pca = clf_no_pca.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the performance without PCA\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
        "print(f'Accuracy without PCA: {accuracy_no_pca:.2f}')\n",
        "\n",
        "# Train a classifier with PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2 principal components for simplicity\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_with_pca = RandomForestClassifier(random_state=42)\n",
        "clf_with_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions with PCA\n",
        "y_pred_with_pca = clf_with_pca.predict(X_test_pca)\n",
        "\n",
        "# Evaluate the performance with PCA\n",
        "accuracy_with_pca = accuracy_score(y_test, y_pred_with_pca)\n",
        "print(f'Accuracy with PCA: {accuracy_with_pca:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Aya3Yrjjr9",
        "outputId": "c8a85543-62c5-4db3-fd35-4f7153f64364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without PCA: 1.00\n",
            "Accuracy with PCA: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate synthetic data with highly correlated features\n",
        "X, y = make_classification(n_samples=1000, n_features=50, n_informative=20, n_redundant=30, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier without PCA\n",
        "clf_no_pca = RandomForestClassifier(random_state=42)\n",
        "clf_no_pca.fit(X_train, y_train)\n",
        "y_pred_no_pca = clf_no_pca.predict(X_test)\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
        "print(f'Accuracy without PCA: {accuracy_no_pca:.2f}')\n",
        "\n",
        "# Train a RandomForestClassifier with PCA\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=5)  # Reduce to 5 principal components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_with_pca = RandomForestClassifier()\n",
        "clf_with_pca.fit(X_train_pca, y_train)\n",
        "y_pred_with_pca = clf_with_pca.predict(X_test_pca)\n",
        "accuracy_with_pca = accuracy_score(y_test, y_pred_with_pca)\n",
        "print(f'Accuracy with PCA: {accuracy_with_pca:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMpsZvCNkFjB",
        "outputId": "f3ae7036-c2ba-4001-a247-9da21f4b65a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without PCA: 0.91\n",
            "Accuracy with PCA: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's adjust the scenario to demonstrate a case where the accuracy is high with PCA or at least comparable and low without PCA. We can modify the synthetic dataset to have features with noise and high dimensionality, making it challenging for the classifier to learn from the original feature space. PCA can help in reducing the noise and focusing on the most informative components."
      ],
      "metadata": {
        "id": "gHIEN9kwlsEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate synthetic data with noise and high dimensionality\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=10, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier without PCA\n",
        "clf_no_pca = RandomForestClassifier(random_state=42)\n",
        "clf_no_pca.fit(X_train, y_train)\n",
        "y_pred_no_pca = clf_no_pca.predict(X_test)\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
        "print(f'Accuracy without PCA: {accuracy_no_pca:.2f}')\n",
        "\n",
        "# Train a RandomForestClassifier with PCA\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=10)  # Reduce to 10 principal components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_with_pca = RandomForestClassifier(random_state=42)\n",
        "clf_with_pca.fit(X_train_pca, y_train)\n",
        "y_pred_with_pca = clf_with_pca.predict(X_test_pca)\n",
        "accuracy_with_pca = accuracy_score(y_test, y_pred_with_pca)\n",
        "print(f'Accuracy with PCA: {accuracy_with_pca:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbrtfcD3kizf",
        "outputId": "f4e3c735-d12f-43ee-bb98-d18619ada7e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without PCA: 0.88\n",
            "Accuracy with PCA: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "# Given transactions\n",
        "transactions = [[\"Milk\", \"Bread\", \"Eggs\", \"Cheese\", \"Yogurt\"],\n",
        "                [\"Milk\", \"Bread\", \"Cheese\", \"Yogurt\"],\n",
        "                [\"Milk\", \"Bread\", \"Butter\", \"Juice\"],\n",
        "                [\"Eggs\", \"Bread\", \"Cheese\", \"Juice\"],\n",
        "                [\"Milk\", \"Bread\", \"Butter\", \"Cheese\", \"Jam\", \"Juice\"],\n",
        "                [\"Milk\", \"Eggs\", \"Jam\", \"Yogurt\"]]\n",
        "\n",
        "# Create a list of unique items\n",
        "items = list(set(item for sublist in transactions for item in sublist))\n",
        "\n",
        "# Create a dictionary to map items to indices\n",
        "item_to_index = {item: index for index, item in enumerate(items)}\n",
        "index_to_item = {index: item for item, index in item_to_index.items()}\n",
        "\n",
        "# Create a transaction matrix\n",
        "num_transactions = len(transactions)\n",
        "num_items = len(items)\n",
        "transaction_matrix = np.zeros((num_transactions, num_items), dtype=float)  # Change dtype to float\n",
        "\n",
        "for i, transaction in enumerate(transactions):\n",
        "    for item in transaction:\n",
        "        j = item_to_index[item]\n",
        "        transaction_matrix[i, j] = 1\n",
        "\n",
        "# Convert the transaction matrix to a sparse matrix\n",
        "sparse_transaction_matrix = csr_matrix(transaction_matrix)\n",
        "\n",
        "# Perform Singular Value Decomposition (SVD)\n",
        "k = min(num_transactions, num_items) - 1\n",
        "U, Sigma, VT = svds(sparse_transaction_matrix, k=k)\n",
        "\n",
        "# Generate recommendations\n",
        "def recommend_items(user_id, U, Sigma, VT, top_n=3):\n",
        "    user_vector = U[user_id, :]\n",
        "    scores = np.dot(np.dot(user_vector, np.diag(Sigma)), VT)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
        "    top_items = [index_to_item[index] for index in top_indices]\n",
        "    return top_items\n",
        "\n",
        "# Evaluate the model's performance (Example)\n",
        "# Here, we'll evaluate the recommendation for the first user\n",
        "user_id = 0\n",
        "recommended_items = recommend_items(user_id, U, Sigma, VT)\n",
        "print(\"Recommended items for user\", user_id, \":\", recommended_items)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aj7paxhlTKi",
        "outputId": "f96770b9-8a91-4638-fdc3-815bbec72efa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended items for user 0 : ['Yogurt', 'Bread', 'Milk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LJ2_uVW3Wa-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}